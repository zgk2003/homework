{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (Hinge Loss vs Logistic Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to clean up the csv file by pushing the 1st data point (which is incorrectly encoded as columns) into the dataframe and then renaming columns, with the target feature called 'Feature 0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('breast-cancer.csv')\n",
    "df.loc[-1] = [float(i) for i in df.columns] \n",
    "df.index = df.index + 1\n",
    "df = df.sort_index()\n",
    "df.columns = ['Feature ' + str(i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this helper function to print out the coefficients of the intercept and weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_coefs(results):\n",
    "    print('Intercept coefficient:\\t', results.intercept_)\n",
    "    for i in range(len(df.columns[:-1])):\n",
    "        print('Coefficient of', df.columns[i], ':\\t', results.coef_[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>683 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature 0  Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  \\\n",
       "0         -1.0        5.0        1.0        1.1        1.2        2.0   \n",
       "1         -1.0        5.0        4.0        4.0        5.0        7.0   \n",
       "2         -1.0        3.0        1.0        1.0        1.0        2.0   \n",
       "3         -1.0        6.0        8.0        8.0        1.0        3.0   \n",
       "4         -1.0        4.0        1.0        1.0        3.0        2.0   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "678       -1.0        3.0        1.0        1.0        1.0        3.0   \n",
       "679       -1.0        2.0        1.0        1.0        1.0        2.0   \n",
       "680        1.0        5.0       10.0       10.0        3.0        7.0   \n",
       "681        1.0        4.0        8.0        6.0        4.0        3.0   \n",
       "682        1.0        4.0        8.0        8.0        5.0        4.0   \n",
       "\n",
       "     Feature 6  Feature 7  Feature 8  Feature 9  \n",
       "0          1.3        3.0        1.4        1.5  \n",
       "1         10.0        3.0        2.0        1.0  \n",
       "2          2.0        3.0        1.0        1.0  \n",
       "3          4.0        3.0        7.0        1.0  \n",
       "4          1.0        3.0        1.0        1.0  \n",
       "..         ...        ...        ...        ...  \n",
       "678        2.0        1.0        1.0        1.0  \n",
       "679        1.0        1.0        1.0        1.0  \n",
       "680        3.0        8.0       10.0        2.0  \n",
       "681        4.0       10.0        6.0        1.0  \n",
       "682        5.0       10.0        4.0        1.0  \n",
       "\n",
       "[683 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Shuffle all of the data points using df.sample() with a random state of 42, then split the data set into a 50/25/25 training/validation/test split. So, the first half of the shuffled data set will be the training set, the 3rd quarter will be the validation set, and the 4th quarter will be the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(df.shape[0], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = df.iloc[:int(df.shape[0]*0.5), :]\n",
    "va = df.iloc[int(df.shape[0]*0.5):int(df.shape[0]*0.75), :]\n",
    "te = df.iloc[int(df.shape[0]*0.75):, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Using validation to determine an appropriate regularization parameter $\\lambda$, fit a linear support vector classifier (SVC, `sklearn.svm.SVC`) and logistic regressor (LR, `sklearn.linear_model.LogisticRegression`) for 50 iterations to minimize the regularized empirical risk $$\\min \\frac{1}{n} \\sum_{i=1}^{n} \\ell(x_i,y;w) + \\lambda \\cdot ||w||_2^2$$\n",
    "\n",
    "for a hinge loss and logistic loss, respectively. \n",
    "\n",
    "In particular, for each $\\lambda \\in \\Lambda =  \\{.01,.02,.03,...,1\\}$, fit a SVC and LR with appropriate inputs to (possibly a subset of) arguments  { `penalty` , `loss` , `C` , `max_iter`, `random_state`, `fit_intercept`}, and then compute the misclassification error rate on the validation set. Use `max_iter=100000` and `random_state=42` for all trained models, and be sure to add an offset (`fit_intercept=True`) to your model. \n",
    "\n",
    "(Hint: Looking at the documentation for these models in sklearn may help.)\n",
    "\n",
    "__Print out the best regularization parameter $\\lambda^* \\in \\Lambda$ (using the `print_coefs` helper function) and the intercept & weight coefficients for SVC and LR models corresponding to $\\lambda^*$__ \n",
    "\n",
    "We will call these weight vectors $w^*_\\text{hinge}$ and $w^*_\\text{logistic}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err(y_hat, y):\n",
    "    return np.mean(y_hat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmbdas = np.arange(0.01, 1.00005, 0.01)\n",
    "lmbdas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_svm = np.zeros(100,)\n",
    "for i in range(lmbdas.shape[0]):\n",
    "    # C is the inverse of lmbda\n",
    "    # A larger max_iter is used since the smaller lambda will result in Python warnings about non-convergence\n",
    "    clf = SVC(C = 1/lmbdas[i], max_iter = 10000000, kernel = 'linear', random_state = 42)\n",
    "    clf.fit(tr.drop(['Feature 0'], axis = 1), tr['Feature 0'])\n",
    "    preds = clf.predict(va.drop(['Feature 0'], axis = 1))\n",
    "    error = err(preds, va['Feature 0'])\n",
    "    errors_svm[i] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmin(errors_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lmbda_svm = lmbdas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization parameter via validation for the SVM:\t 0.01\n"
     ]
    }
   ],
   "source": [
    "print('Best regularization parameter via validation for the SVM:\\t', best_lmbda_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_lr = np.zeros(100,)\n",
    "for i in range(lmbdas.shape[0]):\n",
    "    # C is the inverse of lmbda\n",
    "    clf = LogisticRegression(C = 1/lmbdas[i], fit_intercept = True, max_iter = 100000, random_state = 42)\n",
    "    clf.fit(tr.drop(['Feature 0'], axis = 1), tr['Feature 0']) \n",
    "    preds = clf.predict(va.drop(['Feature 0'], axis = 1))\n",
    "    error = err(preds, va['Feature 0'])\n",
    "    errors_lr[i] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmin(errors_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lmbda_lr = lmbdas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization parameter via validation for logistic Regression:\t 0.13\n"
     ]
    }
   ],
   "source": [
    "print('Best regularization parameter via validation for logistic Regression:\\t', best_lmbda_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, kernel='linear', max_iter=10000000, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C = 1/best_lmbda_svm, max_iter = 10000000, kernel = 'linear', random_state = 42)\n",
    "svm.fit(tr.drop(['Feature 0'], axis = 1), tr['Feature 0']) # C is the inverse of lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept and Weights learned for Linear SVM model:\n",
      "\n",
      "Intercept coefficient:\t [-6.62274282]\n",
      "Coefficient of Feature 0 :\t 0.4327403953729032\n",
      "Coefficient of Feature 1 :\t -0.11566254991596381\n",
      "Coefficient of Feature 2 :\t 0.5349819782606353\n",
      "Coefficient of Feature 3 :\t 0.2859318738718297\n",
      "Coefficient of Feature 4 :\t 0.14539437478627626\n",
      "Coefficient of Feature 5 :\t 0.30092756740900306\n",
      "Coefficient of Feature 6 :\t -0.09394667827241321\n",
      "Coefficient of Feature 7 :\t 0.20228115456548945\n",
      "Coefficient of Feature 8 :\t 0.43461420929315064\n"
     ]
    }
   ],
   "source": [
    "print('Intercept and Weights learned for Linear SVM model:\\n')\n",
    "print_coefs(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=7.692307692307692, max_iter=1000000, random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C = 1/best_lmbda_lr, fit_intercept = True, max_iter = 1000000, random_state = 42)\n",
    "lr.fit(tr.drop(['Feature 0'], axis = 1), tr['Feature 0']) # C is the inverse of lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept and Weights learned for Logistic Regression model:\n",
      "\n",
      "Intercept coefficient:\t [-13.24494534]\n",
      "Coefficient of Feature 0 :\t 0.841718885448195\n",
      "Coefficient of Feature 1 :\t 0.21131888783035613\n",
      "Coefficient of Feature 2 :\t 1.1259693843248286\n",
      "Coefficient of Feature 3 :\t 0.3819520202726725\n",
      "Coefficient of Feature 4 :\t 0.03295039194701652\n",
      "Coefficient of Feature 5 :\t 0.4154922872396178\n",
      "Coefficient of Feature 6 :\t 0.1810764173361439\n",
      "Coefficient of Feature 7 :\t 0.2636751379463769\n",
      "Coefficient of Feature 8 :\t 0.2894598197882617\n"
     ]
    }
   ],
   "source": [
    "print('Intercept and Weights learned for Logistic Regression model:\\n') \n",
    "print_coefs(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) \n",
    "\n",
    "Report the misclassification rates of $w^*_\\text{hinge}$ and $w^*_\\text{logistic}$ on\n",
    "the test set. Which model performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error of the SVM, which is built upon the hinge loss, is 0.029.\n",
      "The error of the Logistic Regression, which is built upon the logistic loss, is 0.035.\n"
     ]
    }
   ],
   "source": [
    "preds_svm = svm.predict(te.drop(['Feature 0'], axis = 1))\n",
    "preds_lr = lr.predict(te.drop(['Feature 0'], axis = 1))\n",
    "\n",
    "print(\"The error of the SVM, which is built upon the hinge loss, is \" + \n",
    "      str(round(err(preds_svm, te['Feature 0']), 3)) + \".\")\n",
    "print(\"The error of the Logistic Regression, which is built upon the logistic loss, is \" + \n",
    "      str(round(err(preds_lr, te['Feature 0']), 3)) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) See pdf.\n",
    "<img src=\"image.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Compute the log-likelihood of these two models using the test data set. Creating some helper functions that compute hinge loss and logistic loss for a given data point and weight vector $(x,y,w)$ may help.\n",
    "\n",
    "Relate your findings to your misclassification error results in part (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge(y_pred, y):\n",
    "    if 1 - y_pred * y > 0:\n",
    "        return 1 - y_pred * y\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def logistic(y_pred, y):\n",
    "    loss = np.log(1 + np.exp(- y * y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(y_pred, y, loss):\n",
    "    if loss == 'logistic':\n",
    "        return - logistic(y_pred, y)\n",
    "    else:\n",
    "        z = np.exp(-hinge(y_pred, 1)) + np.exp(-hinge(y_pred, -1))\n",
    "        return - hinge(y_pred, y) + np.log(1/z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0181499279178094"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob(3,-1,'hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average log likelihood of the true label for each test point given the SVM prediction is: -0.18540754320671582\n",
      "The average log likelihood of the true label for each test point given the LR prediction is: -0.3483494068164679\n"
     ]
    }
   ],
   "source": [
    "hinge_total = 0\n",
    "logistic_total = 0\n",
    "for i in range(te.shape[0]):\n",
    "    pred_svm = preds_svm[i]\n",
    "    pred_lr = preds_lr[i]\n",
    "    y = te.iat[i, 0]\n",
    "    hinge_total += log_prob(pred_svm, y, 'hinge')\n",
    "    logistic_total += log_prob(pred_lr, y, 'logistic')\n",
    "avg_hinge = hinge_total / te.shape[0]\n",
    "avg_logistic = logistic_total / te.shape[0]\n",
    "print(\"The average log likelihood of the true label for each test point given the SVM prediction is: \" + str(avg_hinge))\n",
    "print(\"The average log likelihood of the true label for each test point given the LR prediction is: \" + str(avg_logistic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Response:__ The average log likelihood of the SVM if higher than that of the logistic regression, meaning that on average the SVM is more confident about its predictions. This is why the SVM's error is lower in the last part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
